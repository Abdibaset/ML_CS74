{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdibaset/ML_CS74/blob/main/MultiClassClassification_Project3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Metrics**"
      ],
      "metadata": {
        "id": "2Kl71ED7VDNq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "6E51-CrBDlzM"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, roc_auc_score, accuracy_score, f1_score\n",
        "import numpy as np\n",
        "\n",
        "def evaluationMetrics(y_actual, y_predicted, valid_set=None, model=None):\n",
        "  y_actual = np.array(y_actual).reshape(-1, 1)  # Flatten the array\n",
        "  y_predicted = np.array(y_predicted).reshape(-1, 1)\n",
        "\n",
        "  if model and valid_set is not None:\n",
        "    y_predicted_probabilities = model.predict_proba(valid_set)\n",
        "  else:\n",
        "    y_predicted_probabilities = None\n",
        "\n",
        "  ## get metric scores\n",
        "  confusion_matrix_result = confusion_matrix(y_actual, y_predicted)\n",
        "  roc_auc_per_class = roc_auc_score(y_actual, y_predicted_probabilities, multi_class=\"ovr\", average=None)\n",
        "  macro_roc_auc = roc_auc_score(y_actual, y_predicted_probabilities, multi_class=\"ovr\", average=\"macro\")\n",
        "  accuracy_result = accuracy_score(y_actual, y_predicted)\n",
        "  f1_score_result = f1_score(y_actual, y_predicted, average=\"macro\")\n",
        "\n",
        "  print(\"Confusion Matrix: \", confusion_matrix_result)\n",
        "  print(\"roc_auc per class: \", roc_auc_per_class)\n",
        "  print(\"macro_roc_auc\", macro_roc_auc)\n",
        "  print(\"accuracy: \", accuracy_result)\n",
        "  print(\"f1_score: \", f1_score_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing data**"
      ],
      "metadata": {
        "id": "pbCW-QZRVHO-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4QQ7Z9WDnvG",
        "outputId": "bf9abac3-1033-40a6-c3c6-44525750e073"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    5959\n",
              "1    5957\n",
              "3    5862\n",
              "4    5769\n",
              "5    5642\n",
              "Name: overall, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 191
        }
      ],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "train_data = train_data[['summary', 'reviewText', 'overall', 'verified', 'category']]\n",
        "test_data = test_data[['summary', 'reviewText', 'id', 'verified', 'category']]\n",
        "\n",
        "important_text_features_train = train_data['summary'].fillna('') + ' ' + train_data['reviewText'].fillna('')\n",
        "important_text_features_test =  test_data['summary'].fillna('')+ ' ' + test_data['reviewText'].fillna('')\n",
        "\n",
        "target_label = train_data['overall']\n",
        "train_data['overall'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JI68-Zm4DuRM",
        "outputId": "a9a75dde-d7fd-4485-c443-bd18276f4609"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def preprocess(text):\n",
        "  tokens = word_tokenize(text) # tokenize with nltk\n",
        "  tokens = [w.lower() for w in tokens] # convert to all lower case\n",
        "\n",
        "  # stem and then lemmatize data\n",
        "  tokens = [lemmatizer.lemmatize(stemmer.stem(w)) for w in tokens if  w not in string.punctuation]\n",
        "  return ' '.join(tokens)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "KDC7h1qTDwpw"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# vectorization of data\n",
        "vectorizer = TfidfVectorizer(\n",
        "    ngram_range=(1, 3),\n",
        "    sublinear_tf=True\n",
        ")\n",
        "\n",
        "## fit into the vectorizer\n",
        "train_text_features = vectorizer.fit_transform(important_text_features_train)\n",
        "test_text_features = vectorizer.transform(important_text_features_test)\n",
        "\n",
        "## split the data while maintaining the distribution\n",
        "train_set, validation_set, train_target, validation_target = train_test_split(\n",
        "    train_text_features, target_label, test_size=0.2, random_state=27, stratify=target_label\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression Model**"
      ],
      "metadata": {
        "id": "Byr71MtjVN7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI4-ZZcNOEep",
        "outputId": "0bf479ed-fd9e-42d4-f808-6a75405521f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.72      0.68      1192\n",
            "           2       0.53      0.49      0.51      1192\n",
            "           3       0.55      0.53      0.54      1172\n",
            "           4       0.57      0.56      0.57      1154\n",
            "           5       0.71      0.74      0.72      1128\n",
            "\n",
            "    accuracy                           0.61      5838\n",
            "   macro avg       0.60      0.61      0.60      5838\n",
            "weighted avg       0.60      0.61      0.60      5838\n",
            "\n",
            "Confusion Matrix:  [[854 218  61  37  22]\n",
            " [280 583 244  61  24]\n",
            " [101 205 618 197  51]\n",
            " [ 35  65 166 652 236]\n",
            " [ 38  29  41 190 830]]\n",
            "accuracy:  0.605858170606372\n",
            "f1_score:  0.6045217202485745\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "logreg = LogisticRegression()\n",
        "params = {\n",
        "    \"multi_class\": [\"ovr\"],\n",
        "    \"C\": [2.5, 3.5, 4.0],\n",
        "    \"solver\": ['lbfgs', 'saga'],\n",
        "    \"class_weight\": ['balanced'],\n",
        "    \"max_iter\": [1000, 1500],\n",
        "    \"penalty\": ['l2']\n",
        "}\n",
        "\n",
        "## cross-validation with gridsearchcv\n",
        "grid = GridSearchCV(logreg, params, cv=5, scoring=\"f1_macro\")\n",
        "grid.fit(train_set, train_target)\n",
        "best_log = grid.best_estimator_\n",
        "\n",
        "# Fit the best estimator on the training data\n",
        "best_log.fit(train_set, train_target)\n",
        "validation_pred = best_log.predict(validation_set) # predict on the validation set\n",
        "\n",
        "# print report of predictions of best classifier on the validation set\n",
        "print(classification_report(validation_target, validation_pred))\n",
        "evaluationMetrics(validation_target, validation_pred, validation_set, best_log)\n",
        "\n",
        "## predict test and save data\n",
        "test_predicted = best_log.predict(test_text_features)\n",
        "logReg_submission = {'pred': test_predicted, 'id':test_data['id']}\n",
        "logReg_submission_df = pd.DataFrame(logReg_submission)\n",
        "logReg_submission_df.to_csv('log_reg.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Multinomial NaiveBayes Model**"
      ],
      "metadata": {
        "id": "MlL2WiRGVSyh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "Multi_NB = MultinomialNB()\n",
        "params = {\n",
        "    \"alpha\": [0.1, 0.5, 1.0, 2.0],\n",
        "    \"fit_prior\": [True, False]\n",
        "}\n",
        "\n",
        "# Cross-validation with grid search\n",
        "grid = GridSearchCV(Multi_NB, params, cv=5, scoring=\"f1_macro\")\n",
        "grid.fit(train_set, train_target)\n",
        "best_multi_nb = grid.best_estimator_\n",
        "best_multi_nb.fit(train_set, train_target)\n",
        "validation_pred = best_multi_nb.predict(validation_set)\n",
        "\n",
        "## print report on the validation predictions\n",
        "print(\"Classification Report for Multinomial Naive Bayes:\\n\", classification_report(validation_target, validation_pred))\n",
        "evaluationMetrics(validation_target, validation_pred, validation_set, best_multi_nb)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predicted = best_multi_nb.predict(test_text_features)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "mnb_submission = {'pred': test_predicted, 'id': test_data['id']}\n",
        "mnb_submission_df = pd.DataFrame(mnb_submission)\n",
        "mnb_submission_df.to_csv('multinomial_nb_submission.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMixDSQL-06y",
        "outputId": "e1157580-eca2-44d9-e9b4-25c06c6a381f"
      },
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Multinomial Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.73      0.59      0.65      1192\n",
            "           2       0.44      0.61      0.51      1192\n",
            "           3       0.48      0.52      0.50      1172\n",
            "           4       0.55      0.61      0.58      1154\n",
            "           5       0.86      0.51      0.64      1128\n",
            "\n",
            "    accuracy                           0.57      5838\n",
            "   macro avg       0.61      0.57      0.58      5838\n",
            "weighted avg       0.61      0.57      0.58      5838\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random forest Model**"
      ],
      "metadata": {
        "id": "TMe5vBAdVbWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "\n",
        "rf_classifier = RandomForestClassifier(random_state=27)\n",
        "params = {\n",
        "    'n_estimators': [100],\n",
        "    'max_depth': [10],\n",
        "    'min_samples_split': [5],\n",
        "    # 'class_weight': [{1: 1, 2: 2, 3: 1.5, 4: 2.5, 5: 1}]\n",
        "}\n",
        "\n",
        "\n",
        "grid = GridSearchCV(rf_classifier, params, cv=5, scoring=\"f1_macro\")\n",
        "grid.fit(train_set, train_target)\n",
        "best_rf = grid.best_estimator_\n",
        "best_rf.fit(train_set, train_target)\n",
        "\n",
        "validation_pred = best_rf.predict(validation_set)\n",
        "print(\"Classification Report for Random Forest:\\n\", classification_report(validation_target, validation_pred))\n",
        "evaluationMetrics(validation_target, validation_pred, validation_set, best_rf)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predicted = best_rf.predict(test_text_features)\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "rf_submission = {'pred': test_predicted, 'id': test_data['id']}\n",
        "rf_submission_df = pd.DataFrame(rf_submission)\n",
        "rf_submission_df.to_csv('random_forest_submission.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1EPkHWNA1Ep",
        "outputId": "9ee97b57-aef3-4bfa-b5f7-28ab7dae391a"
      },
      "execution_count": 237,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           1       0.41      0.72      0.52      1192\n",
            "           2       0.32      0.41      0.36      1192\n",
            "           3       0.49      0.41      0.45      1172\n",
            "           4       0.47      0.20      0.28      1154\n",
            "           5       0.69      0.43      0.53      1128\n",
            "\n",
            "    accuracy                           0.44      5838\n",
            "   macro avg       0.47      0.43      0.43      5838\n",
            "weighted avg       0.47      0.44      0.43      5838\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyP+Dgnp9MbXMPsf86HR/VU0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}